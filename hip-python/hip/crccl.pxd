# MIT License
#
# Copyright (c) 2023-2024 Advanced Micro Devices, Inc.
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# This file has been autogenerated, do not modify.

from libc.stdint cimport *
ctypedef bint _Bool # bool is not a reserved keyword in C, _Bool is
from .chip cimport hipStream_t
cdef extern from "rccl/rccl.h":

    cdef int NCCL_MAJOR

    cdef int NCCL_MINOR

    cdef int NCCL_PATCH

    cdef char * NCCL_SUFFIX

    cdef int NCCL_VERSION_CODE

    cdef int RCCL_BFLOAT16

    cdef int RCCL_GATHER_SCATTER

    cdef int RCCL_ALLTOALLV

    cdef int NCCL_UNIQUE_ID_BYTES

    cdef struct ncclComm:
        pass

    ctypedef ncclComm * ncclComm_t

    ctypedef struct ncclUniqueId:
        char[128] internal

    ctypedef enum ncclResult_t:
        ncclSuccess
        ncclUnhandledCudaError
        ncclSystemError
        ncclInternalError
        ncclInvalidArgument
        ncclInvalidUsage
        ncclRemoteError
        ncclInProgress
        ncclNumResults

    cdef struct ncclConfig_v21700:
        unsigned long size
        unsigned int magic
        unsigned int version
        int blocking
        int cgaClusterSize
        int minCTAs
        int maxCTAs
        const char * netName
        int splitShare

    ctypedef ncclConfig_v21700 ncclConfig_t

# @} */
cdef ncclResult_t ncclMemAlloc(void ** ptr,unsigned long size)



cdef ncclResult_t pncclMemAlloc(void ** ptr,unsigned long size)



cdef ncclResult_t ncclMemFree(void * ptr)



cdef ncclResult_t pncclMemFree(void * ptr)


# @brief      Return the RCCL_VERSION_CODE of RCCL in the supplied integer.
# @details    This integer is coded with the MAJOR, MINOR and PATCH level of RCCL.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[out] version       Pointer to where version will be stored
cdef ncclResult_t ncclGetVersion(int * version)


# @cond       include_hidden */
cdef ncclResult_t pncclGetVersion(int * version)


# @brief      Generates an ID for ncclCommInitRank.
# @details    Generates an ID to be used in ncclCommInitRank.
#             ncclGetUniqueId should be called once by a single rank and the
#             ID should be distributed to all ranks in the communicator before
#             using it as a parameter for ncclCommInitRank.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[out] uniqueId      Pointer to where uniqueId will be stored
cdef ncclResult_t ncclGetUniqueId(ncclUniqueId * uniqueId)


# @cond       include_hidden */
cdef ncclResult_t pncclGetUniqueId(ncclUniqueId * uniqueId)


# @brief      Create a new communicator with config.
# @details    Create a new communicator (multi thread/process version) with a configuration
#             set by users. See @ref rccl_config_type for more details.
#             Each rank is associated to a CUDA device, which has to be set before calling
#             ncclCommInitRank.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[out] comm          Pointer to created communicator
# @param[in]  nranks        Total number of ranks participating in this communicator
# @param[in]  commId        UniqueId required for initialization
# @param[in]  rank          Current rank to create communicator for. [0 to nranks-1]
# @param[in]  config        Pointer to communicator configuration
cdef ncclResult_t ncclCommInitRankConfig(ncclComm_t* comm,int nranks,ncclUniqueId commId,int rank,ncclConfig_v21700 * config)


# @cond       include_hidden */
cdef ncclResult_t pncclCommInitRankConfig(ncclComm_t* comm,int nranks,ncclUniqueId commId,int rank,ncclConfig_v21700 * config)


# @brief      Creates a new communicator (multi thread/process version).
# @details    Rank must be between 0 and nranks-1 and unique within a communicator clique.
#             Each rank is associated to a CUDA device, which has to be set before calling
#             ncclCommInitRank.  ncclCommInitRank implicitly syncronizes with other ranks,
#             so it must be called by different threads/processes or use ncclGroupStart/ncclGroupEnd.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[out] comm          Pointer to created communicator
# @param[in]  nranks        Total number of ranks participating in this communicator
# @param[in]  commId        UniqueId required for initialization
# @param[in]  rank          Current rank to create communicator for
cdef ncclResult_t ncclCommInitRank(ncclComm_t* comm,int nranks,ncclUniqueId commId,int rank)


# @cond       include_hidden */
cdef ncclResult_t pncclCommInitRank(ncclComm_t* comm,int nranks,ncclUniqueId commId,int rank)


# @brief      Creates a clique of communicators (single process version).
# @details    This is a convenience function to create a single-process communicator clique.
#             Returns an array of ndev newly initialized communicators in comm.
#             comm should be pre-allocated with size at least ndev*sizeof(ncclComm_t).
#             If devlist is NULL, the first ndev HIP devices are used.
#             Order of devlist defines user-order of processors within the communicator.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[out] comm          Pointer to array of created communicators
# @param[in]  ndev          Total number of ranks participating in this communicator
# @param[in]  devlist       Array of GPU device indices to create for
cdef ncclResult_t ncclCommInitAll(ncclComm_t* comm,int ndev,const int * devlist)


# @cond       include_hidden */
cdef ncclResult_t pncclCommInitAll(ncclComm_t* comm,int ndev,const int * devlist)


# @brief      Finalize a communicator.
# @details    ncclCommFinalize flushes all issued communications
#             and marks communicator state as ncclInProgress. The state will change to ncclSuccess
#             when the communicator is globally quiescent and related resources are freed; then,
#             calling ncclCommDestroy can locally free the rest of the resources (e.g. communicator
#             itself) without blocking.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  comm          Communicator to finalize
cdef ncclResult_t ncclCommFinalize(ncclComm_t comm)


# @cond       include_hidden */
cdef ncclResult_t pncclCommFinalize(ncclComm_t comm)


# @brief      Frees local resources associated with communicator object.
# @details    Destroy all local resources associated with the passed in communicator object
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  comm          Communicator to destroy
cdef ncclResult_t ncclCommDestroy(ncclComm_t comm)


# @cond       include_hidden */
cdef ncclResult_t pncclCommDestroy(ncclComm_t comm)


# @brief      Abort any in-progress calls and destroy the communicator object.
# @details    Frees resources associated with communicator object and aborts any operations
#             that might still be running on the device.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  comm          Communicator to abort and destroy
cdef ncclResult_t ncclCommAbort(ncclComm_t comm)


# @cond       include_hidden */
cdef ncclResult_t pncclCommAbort(ncclComm_t comm)


# @brief      Create one or more communicators from an existing one.
# @details    Creates one or more communicators from an existing one.
#             Ranks with the same color will end up in the same communicator.
#             Within the new communicator, key will be used to order ranks.
#             NCCL_SPLIT_NOCOLOR as color will indicate the rank will not be part of any group
#             and will therefore return a NULL communicator.
#             If config is NULL, the new communicator will inherit the original communicator's configuration
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  comm          Original communicator object for this rank
# @param[in]  color         Color to assign this rank
# @param[in]  key           Key used to order ranks within the same new communicator
# @param[out] newcomm       Pointer to new communicator
# @param[in]  config        Config file for new communicator. May be NULL to inherit from comm
cdef ncclResult_t ncclCommSplit(ncclComm_t comm,int color,int key,ncclComm_t* newcomm,ncclConfig_v21700 * config)


# @cond       include_hidden */
cdef ncclResult_t pncclCommSplit(ncclComm_t comm,int color,int key,ncclComm_t* newcomm,ncclConfig_v21700 * config)


# @brief      Returns a string for each result code.
# @details    Returns a human-readable string describing the given result code.
# @return     String containing description of result code.
# 
# @param[in]  result        Result code to get description for
cdef const char * ncclGetErrorString(ncclResult_t result)


# @cond       include_hidden */
cdef const char * pncclGetErrorString(ncclResult_t result)


# @endcond */
cdef const char * ncclGetLastError(ncclComm_t comm)


# @cond       include_hidden */
cdef const char * pncclGetLastError(ncclComm_t comm)


# @brief      Checks whether the comm has encountered any asynchronous errors
# @details    Query whether the provided communicator has encountered any asynchronous errors
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  comm          Communicator to query
# @param[out] asyncError    Pointer to where result code will be stored
cdef ncclResult_t ncclCommGetAsyncError(ncclComm_t comm,ncclResult_t * asyncError)


# @cond       include_hidden */
cdef ncclResult_t pncclCommGetAsyncError(ncclComm_t comm,ncclResult_t * asyncError)


# @brief      Gets the number of ranks in the communicator clique.
# @details    Returns the number of ranks in the communicator clique (as set during initialization)
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  comm          Communicator to query
# @param[out] count         Pointer to where number of ranks will be stored
cdef ncclResult_t ncclCommCount(ncclComm_t comm,int * count)


# @cond       include_hidden */
cdef ncclResult_t pncclCommCount(ncclComm_t comm,int * count)


# @brief      Get the ROCm device index associated with a communicator
# @details    Returns the ROCm device number associated with the provided communicator.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  comm          Communicator to query
# @param[out] device        Pointer to where the associated ROCm device index will be stored
cdef ncclResult_t ncclCommCuDevice(ncclComm_t comm,int * device)


# @cond       include_hidden */
cdef ncclResult_t pncclCommCuDevice(ncclComm_t comm,int * device)


# @brief      Get the rank associated with a communicator
# @details    Returns the user-ordered "rank" associated with the provided communicator.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  comm          Communicator to query
# @param[out] rank          Pointer to where the associated rank will be stored
cdef ncclResult_t ncclCommUserRank(ncclComm_t comm,int * rank)


# @cond       include_hidden */
cdef ncclResult_t pncclCommUserRank(ncclComm_t comm,int * rank)


# @endcond */
# @} */
cdef ncclResult_t ncclCommRegister(ncclComm_t comm,void * buff,unsigned long size,void ** handle)


# @cond       include_hidden */
cdef ncclResult_t pncclCommRegister(ncclComm_t comm,void * buff,unsigned long size,void ** handle)


# @endcond */
cdef ncclResult_t ncclCommDeregister(ncclComm_t comm,void * handle)


# @cond       include_hidden */
cdef ncclResult_t pncclCommDeregister(ncclComm_t comm,void * handle)


cdef extern from "rccl/rccl.h":

    ctypedef enum ncclRedOp_dummy_t:
        ncclNumOps_dummy

    ctypedef enum ncclRedOp_t:
        ncclSum
        ncclProd
        ncclMax
        ncclMin
        ncclAvg
        ncclNumOps
        ncclMaxRedOp

    ctypedef enum ncclDataType_t:
        ncclInt8
        ncclChar
        ncclUint8
        ncclInt32
        ncclInt
        ncclUint32
        ncclInt64
        ncclUint64
        ncclFloat16
        ncclHalf
        ncclFloat32
        ncclFloat
        ncclFloat64
        ncclDouble
        ncclBfloat16
        ncclFp8E4M3
        ncclFp8E5M2
        ncclNumTypes

    ctypedef enum ncclScalarResidence_t:
        ncclScalarDevice
        ncclScalarHostImmediate

# @brief      Create a custom pre-multiplier reduction operator
# @details    Creates a new reduction operator which pre-multiplies input values by a given
#             scalar locally before reducing them with peer values via summation. For use
#             only with collectives launched against *comm* and *datatype*. The
#              residence* argument indicates how/when the memory pointed to by *scalar*
#             will be dereferenced. Upon return, the newly created operator's handle
#             is stored in *op*.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[out] op            Pointer to where newly created custom reduction operator is to be stored
# @param[in]  scalar        Pointer to scalar value.
# @param[in]  datatype      Scalar value datatype
# @param[in]  residence     Memory type of the scalar value
# @param[in]  comm          Communicator to associate with this custom reduction operator
cdef ncclResult_t ncclRedOpCreatePreMulSum(ncclRedOp_t * op,void * scalar,ncclDataType_t datatype,ncclScalarResidence_t residence,ncclComm_t comm)


# @cond       include_hidden */
cdef ncclResult_t pncclRedOpCreatePreMulSum(ncclRedOp_t * op,void * scalar,ncclDataType_t datatype,ncclScalarResidence_t residence,ncclComm_t comm)


# @brief      Destroy custom reduction operator
# @details    Destroys the reduction operator *op*. The operator must have been created by
#             ncclRedOpCreatePreMul with the matching communicator *comm*. An operator may be
#             destroyed as soon as the last RCCL function which is given that operator returns.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  op            Custom reduction operator is to be destroyed
# @param[in]  comm          Communicator associated with this reduction operator
cdef ncclResult_t ncclRedOpDestroy(ncclRedOp_t op,ncclComm_t comm)


# @cond       include_hidden */
cdef ncclResult_t pncclRedOpDestroy(ncclRedOp_t op,ncclComm_t comm)


# @brief      Reduce
# @details    Reduces data arrays of length *count* in *sendbuff* into *recvbuff* using *op*
#             operation.
#              recvbuff* may be NULL on all calls except for root device.
#              root* is the rank (not the HIP device) where data will reside after the
#              operation is complete.
#             In-place operation will happen if sendbuff == recvbuff.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  sendbuff      Local device data buffer to be reduced
# @param[out] recvbuff      Data buffer where result is stored (only for *root* rank).  May be null for other ranks.
# @param[in]  count         Number of elements in every send buffer
# @param[in]  datatype      Data buffer element datatype
# @param[in]  op            Reduction operator type
# @param[in]  root          Rank where result data array will be stored
# @param[in]  comm          Communicator group object to execute on
# @param[in]  stream        HIP stream to execute collective on
cdef ncclResult_t ncclReduce(const void * sendbuff,void * recvbuff,unsigned long count,ncclDataType_t datatype,ncclRedOp_t op,int root,ncclComm_t comm,hipStream_t stream)


# @cond       include_hidden */
cdef ncclResult_t pncclReduce(const void * sendbuff,void * recvbuff,unsigned long count,ncclDataType_t datatype,ncclRedOp_t op,int root,ncclComm_t comm,hipStream_t stream)


# @brief      (Deprecated) Broadcast (in-place)
# @details    Copies *count* values from *root* to all other devices.
#             root is the rank (not the CUDA device) where data resides before the
#             operation is started.
#             This operation is implicitly in-place.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in,out]  buff      Input array on *root* to be copied to other ranks.  Output array for all ranks.
# @param[in]  count         Number of elements in data buffer
# @param[in]  datatype      Data buffer element datatype
# @param[in]  root          Rank owning buffer to be copied to others
# @param[in]  comm          Communicator group object to execute on
# @param[in]  stream        HIP stream to execute collective on
cdef ncclResult_t ncclBcast(void * buff,unsigned long count,ncclDataType_t datatype,int root,ncclComm_t comm,hipStream_t stream)


# @cond       include_hidden */
cdef ncclResult_t pncclBcast(void * buff,unsigned long count,ncclDataType_t datatype,int root,ncclComm_t comm,hipStream_t stream)


# @brief      Broadcast
# @details    Copies *count* values from *sendbuff* on *root* to *recvbuff* on all devices.
#              root* is the rank (not the HIP device) where data resides before the operation is started.
#              sendbuff* may be NULL on ranks other than *root*.
#             In-place operation will happen if *sendbuff* == *recvbuff*.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  sendbuff      Data array to copy (if *root*).  May be NULL for other ranks
# @param[in]  recvbuff      Data array to store received array
# @param[in]  count         Number of elements in data buffer
# @param[in]  datatype      Data buffer element datatype
# @param[in]  root          Rank of broadcast root
# @param[in]  comm          Communicator group object to execute on
# @param[in]  stream        HIP stream to execute collective on
cdef ncclResult_t ncclBroadcast(const void * sendbuff,void * recvbuff,unsigned long count,ncclDataType_t datatype,int root,ncclComm_t comm,hipStream_t stream)


# @cond       include_hidden */
cdef ncclResult_t pncclBroadcast(const void * sendbuff,void * recvbuff,unsigned long count,ncclDataType_t datatype,int root,ncclComm_t comm,hipStream_t stream)


# @brief      All-Reduce
# @details    Reduces data arrays of length *count* in *sendbuff* using *op* operation, and
#             leaves identical copies of result on each *recvbuff*.
#             In-place operation will happen if sendbuff == recvbuff.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  sendbuff      Input data array to reduce
# @param[out] recvbuff      Data array to store reduced result array
# @param[in]  count         Number of elements in data buffer
# @param[in]  datatype      Data buffer element datatype
# @param[in]  op            Reduction operator
# @param[in]  comm          Communicator group object to execute on
# @param[in]  stream        HIP stream to execute collective on
cdef ncclResult_t ncclAllReduce(const void * sendbuff,void * recvbuff,unsigned long count,ncclDataType_t datatype,ncclRedOp_t op,ncclComm_t comm,hipStream_t stream)


# @cond       include_hidden */
cdef ncclResult_t pncclAllReduce(const void * sendbuff,void * recvbuff,unsigned long count,ncclDataType_t datatype,ncclRedOp_t op,ncclComm_t comm,hipStream_t stream)


# @brief      Reduce-Scatter
# @details    Reduces data in *sendbuff* using *op* operation and leaves reduced result
#             scattered over the devices so that *recvbuff* on rank i will contain the i-th
#             block of the result.
#             Assumes sendcount is equal to nranks*recvcount, which means that *sendbuff*
#             should have a size of at least nranks*recvcount elements.
#             In-place operations will happen if recvbuff == sendbuff + rank * recvcount.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  sendbuff      Input data array to reduce
# @param[out] recvbuff      Data array to store reduced result subarray
# @param[in]  recvcount     Number of elements each rank receives
# @param[in]  datatype      Data buffer element datatype
# @param[in]  op            Reduction operator
# @param[in]  comm          Communicator group object to execute on
# @param[in]  stream        HIP stream to execute collective on
cdef ncclResult_t ncclReduceScatter(const void * sendbuff,void * recvbuff,unsigned long recvcount,ncclDataType_t datatype,ncclRedOp_t op,ncclComm_t comm,hipStream_t stream)


# @cond       include_hidden */
cdef ncclResult_t pncclReduceScatter(const void * sendbuff,void * recvbuff,unsigned long recvcount,ncclDataType_t datatype,ncclRedOp_t op,ncclComm_t comm,hipStream_t stream)


# @brief      All-Gather
# @details    Each device gathers *sendcount* values from other GPUs into *recvbuff*,
#             receiving data from rank i at offset i*sendcount.
#             Assumes recvcount is equal to nranks*sendcount, which means that recvbuff
#             should have a size of at least nranks*sendcount elements.
#             In-place operations will happen if sendbuff == recvbuff + rank * sendcount.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  sendbuff      Input data array to send
# @param[out] recvbuff      Data array to store the gathered result
# @param[in]  sendcount     Number of elements each rank sends
# @param[in]  datatype      Data buffer element datatype
# @param[in]  comm          Communicator group object to execute on
# @param[in]  stream        HIP stream to execute collective on
cdef ncclResult_t ncclAllGather(const void * sendbuff,void * recvbuff,unsigned long sendcount,ncclDataType_t datatype,ncclComm_t comm,hipStream_t stream)


# @cond       include_hidden */
cdef ncclResult_t pncclAllGather(const void * sendbuff,void * recvbuff,unsigned long sendcount,ncclDataType_t datatype,ncclComm_t comm,hipStream_t stream)


# @brief      Send
# @details    Send data from *sendbuff* to rank *peer*.
#             Rank *peer* needs to call ncclRecv with the same *datatype* and the same *count*
#             as this rank.
#             This operation is blocking for the GPU. If multiple ncclSend and ncclRecv operations
#             need to progress concurrently to complete, they must be fused within a ncclGroupStart /
#             ncclGroupEnd section.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  sendbuff      Data array to send
# @param[in]  count         Number of elements to send
# @param[in]  datatype      Data buffer element datatype
# @param[in]  peer          Peer rank to send to
# @param[in]  comm          Communicator group object to execute on
# @param[in]  stream        HIP stream to execute collective on
cdef ncclResult_t ncclSend(const void * sendbuff,unsigned long count,ncclDataType_t datatype,int peer,ncclComm_t comm,hipStream_t stream)


# @cond       include_hidden */
cdef ncclResult_t pncclSend(const void * sendbuff,unsigned long count,ncclDataType_t datatype,int peer,ncclComm_t comm,hipStream_t stream)


# @brief      Receive
# @details    Receive data from rank *peer* into *recvbuff*.
#             Rank *peer* needs to call ncclSend with the same datatype and the same count
#             as this rank.
#             This operation is blocking for the GPU. If multiple ncclSend and ncclRecv operations
#             need to progress concurrently to complete, they must be fused within a ncclGroupStart/
#             ncclGroupEnd section.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[out] recvbuff      Data array to receive
# @param[in]  count         Number of elements to receive
# @param[in]  datatype      Data buffer element datatype
# @param[in]  peer          Peer rank to send to
# @param[in]  comm          Communicator group object to execute on
# @param[in]  stream        HIP stream to execute collective on
cdef ncclResult_t ncclRecv(void * recvbuff,unsigned long count,ncclDataType_t datatype,int peer,ncclComm_t comm,hipStream_t stream)


# @cond       include_hidden */
cdef ncclResult_t pncclRecv(void * recvbuff,unsigned long count,ncclDataType_t datatype,int peer,ncclComm_t comm,hipStream_t stream)


# @brief      Gather
# @details    Root device gathers *sendcount* values from other GPUs into *recvbuff*,
#             receiving data from rank i at offset i*sendcount.
#             Assumes recvcount is equal to nranks*sendcount, which means that *recvbuff*
#             should have a size of at least nranks*sendcount elements.
#             In-place operations will happen if sendbuff == recvbuff + rank * sendcount.
#              recvbuff* may be NULL on ranks other than *root*.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  sendbuff      Data array to send
# @param[out] recvbuff      Data array to receive into on *root*.
# @param[in]  sendcount     Number of elements to send per rank
# @param[in]  datatype      Data buffer element datatype
# @param[in]  root          Rank that receives data from all other ranks
# @param[in]  comm          Communicator group object to execute on
# @param[in]  stream        HIP stream to execute collective on
cdef ncclResult_t ncclGather(const void * sendbuff,void * recvbuff,unsigned long sendcount,ncclDataType_t datatype,int root,ncclComm_t comm,hipStream_t stream)


# @cond       include_hidden */
cdef ncclResult_t pncclGather(const void * sendbuff,void * recvbuff,unsigned long sendcount,ncclDataType_t datatype,int root,ncclComm_t comm,hipStream_t stream)


# @brief      Scatter
# @details    Scattered over the devices so that recvbuff on rank i will contain the i-th
#             block of the data on root.
#             Assumes sendcount is equal to nranks*recvcount, which means that *sendbuff*
#             should have a size of at least nranks*recvcount elements.
#             In-place operations will happen if recvbuff == sendbuff + rank * recvcount.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  sendbuff      Data array to send (on *root* rank).  May be NULL on other ranks.
# @param[out] recvbuff      Data array to receive partial subarray into
# @param[in]  recvcount     Number of elements to receive per rank
# @param[in]  datatype      Data buffer element datatype
# @param[in]  root          Rank that scatters data to all other ranks
# @param[in]  comm          Communicator group object to execute on
# @param[in]  stream        HIP stream to execute collective on
cdef ncclResult_t ncclScatter(const void * sendbuff,void * recvbuff,unsigned long recvcount,ncclDataType_t datatype,int root,ncclComm_t comm,hipStream_t stream)


# @cond       include_hidden */
cdef ncclResult_t pncclScatter(const void * sendbuff,void * recvbuff,unsigned long recvcount,ncclDataType_t datatype,int root,ncclComm_t comm,hipStream_t stream)


# @brief      All-To-All
# @details    Device (i) send (j)th block of data to device (j) and be placed as (i)th
#             block. Each block for sending/receiving has *count* elements, which means
#             that *recvbuff* and *sendbuff* should have a size of nranks*count elements.
#             In-place operation is NOT supported. It is the user's responsibility
#             to ensure that sendbuff and recvbuff are distinct.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  sendbuff      Data array to send (contains blocks for each other rank)
# @param[out] recvbuff      Data array to receive (contains blocks from each other rank)
# @param[in]  count         Number of elements to send between each pair of ranks
# @param[in]  datatype      Data buffer element datatype
# @param[in]  comm          Communicator group object to execute on
# @param[in]  stream        HIP stream to execute collective on
cdef ncclResult_t ncclAllToAll(const void * sendbuff,void * recvbuff,unsigned long count,ncclDataType_t datatype,ncclComm_t comm,hipStream_t stream)


# @cond       include_hidden */
cdef ncclResult_t pncclAllToAll(const void * sendbuff,void * recvbuff,unsigned long count,ncclDataType_t datatype,ncclComm_t comm,hipStream_t stream)


# @brief      All-To-Allv
# @details    Device (i) sends sendcounts[j] of data from offset sdispls[j]
#             to device (j). At the same time, device (i) receives recvcounts[j] of data
#             from device (j) to be placed at rdispls[j].
#             sendcounts, sdispls, recvcounts and rdispls are all measured in the units
#             of datatype, not bytes.
#             In-place operation will happen if sendbuff == recvbuff.
# @return     Result code. See @ref rccl_result_code for more details.
# 
# @param[in]  sendbuff      Data array to send (contains blocks for each other rank)
# @param[in]  sendcounts    Array containing number of elements to send to each participating rank
# @param[in]  sdispls       Array of offsets into *sendbuff* for each participating rank
# @param[out] recvbuff      Data array to receive (contains blocks from each other rank)
# @param[in]  recvcounts    Array containing number of elements to receive from each participating rank
# @param[in]  rdispls       Array of offsets into *recvbuff* for each participating rank
# @param[in]  datatype      Data buffer element datatype
# @param[in]  comm          Communicator group object to execute on
# @param[in]  stream        HIP stream to execute collective on
cdef ncclResult_t ncclAllToAllv(const void * sendbuff,const unsigned long* sendcounts,const unsigned long* sdispls,void * recvbuff,const unsigned long* recvcounts,const unsigned long* rdispls,ncclDataType_t datatype,ncclComm_t comm,hipStream_t stream)


# @cond       include_hidden */
cdef ncclResult_t pncclAllToAllv(const void * sendbuff,const unsigned long* sendcounts,const unsigned long* sdispls,void * recvbuff,const unsigned long* recvcounts,const unsigned long* rdispls,ncclDataType_t datatype,ncclComm_t comm,hipStream_t stream)


# @brief      Group Start
# @details    Start a group call. All calls to RCCL until ncclGroupEnd will be fused into
#             a single RCCL operation. Nothing will be started on the HIP stream until
#             ncclGroupEnd.
# @return     Result code. See @ref rccl_result_code for more details.
cdef ncclResult_t ncclGroupStart()


# @cond       include_hidden */
cdef ncclResult_t pncclGroupStart()


# @brief      Group End
# @details    End a group call. Start a fused RCCL operation consisting of all calls since
#             ncclGroupStart. Operations on the HIP stream depending on the RCCL operations
#             need to be called after ncclGroupEnd.
# @return     Result code. See @ref rccl_result_code for more details.
cdef ncclResult_t ncclGroupEnd()


# @cond       include_hidden */
cdef ncclResult_t pncclGroupEnd()
